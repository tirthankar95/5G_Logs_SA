{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eeb4656b-7c1b-40ac-9d44-c40856d0119b",
   "metadata": {},
   "source": [
    "## Method 1\n",
    "### **`Using BERT-type models`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "083f0cba-26b2-40ea-adc5-0938df1d2f3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tirthankar-mittra/ai_ml/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from huggingface_hub import hf_hub_download, list_repo_files\n",
    "from colorama import Style, Fore, Back\n",
    "import torch\n",
    "import os\n",
    "import time\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5305c6-c21d-438b-966f-ab1bbbcb2094",
   "metadata": {},
   "source": [
    "### GET Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a66efb5-71ef-423f-90a2-b8de5c9cb1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_id = \"tabularisai/multilingual-sentiment-analysis\"\n",
    "local_dir = \"/mnt/d/Desktop/HuggingFaceModels/Multi_SA\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d91fae4-d8a2-4a07-b921-e44a1272db88",
   "metadata": {},
   "source": [
    "### DO NOT RUN AGAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfb18c7-972d-4c11-9387-7a3e56c54cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = list_repo_files(repo_id)\n",
    "for filename in filenames:\n",
    "    print(hf_hub_download(repo_id = repo_id, local_dir = local_dir, filename=f\"{filename}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cba7e94f-52c6-4f23-b897-1912492e335b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(local_dir)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(local_dir)\n",
    "\n",
    "def predict_sentiment(text):\n",
    "    input_text = tokenizer(text, return_tensors = \"pt\", \\\n",
    "                           padding = \"max_length\", truncation = True, max_length = 128)\n",
    "    logger.debug(f'{Style.BRIGHT}{Fore.BLUE}{input_text}{Style.RESET_ALL}')\n",
    "    with torch.no_grad():\n",
    "        output_ = model(**input_text)\n",
    "        logger.debug(f'{Style.BRIGHT}{Fore.GREEN}{output_}{Style.RESET_ALL}')\n",
    "    prob = torch.nn.functional.softmax(output_.logits, dim = -1)\n",
    "    sentiment_map = {0: \"VERY BAD\", 1: \"BAD\", 2: \"NEUTRAL\", 3: \"GOOD\", 4: \"VERY GOOD\"}\n",
    "    return [sentiment_map[p] for p in torch.argmax(prob, dim = -1).tolist()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18444840-d04a-4df8-a258-5da265d156a3",
   "metadata": {},
   "source": [
    "### RUN 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a19f446-fd24-445e-869c-53b11b45d2c8",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at alloc_cpu.cpp:117] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 19660800000 bytes. Error code 12 (Cannot allocate memory)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m file:\n\u001b[1;32m      5\u001b[0m         sentences\u001b[38;5;241m.\u001b[39mappend(line)\n\u001b[0;32m----> 7\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_sentiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence, verdict \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sentences, result):\n\u001b[1;32m      9\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mStyle\u001b[38;5;241m.\u001b[39mBRIGHT\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mVerdict: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mverdict\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mStyle\u001b[38;5;241m.\u001b[39mRESET_ALL\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[4], line 9\u001b[0m, in \u001b[0;36mpredict_sentiment\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      7\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mStyle\u001b[38;5;241m.\u001b[39mBRIGHT\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mFore\u001b[38;5;241m.\u001b[39mBLUE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00minput_text\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mStyle\u001b[38;5;241m.\u001b[39mRESET_ALL\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 9\u001b[0m     output_ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mStyle\u001b[38;5;241m.\u001b[39mBRIGHT\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mFore\u001b[38;5;241m.\u001b[39mGREEN\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00moutput_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mStyle\u001b[38;5;241m.\u001b[39mRESET_ALL\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     11\u001b[0m prob \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftmax(output_\u001b[38;5;241m.\u001b[39mlogits, dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/ai_ml/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ai_ml/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/ai_ml/lib/python3.12/site-packages/transformers/models/distilbert/modeling_distilbert.py:978\u001b[0m, in \u001b[0;36mDistilBertForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    970\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    971\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m    972\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m    973\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m    974\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m    975\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    976\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m--> 978\u001b[0m distilbert_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistilbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    979\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    981\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    982\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    984\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    985\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    986\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    987\u001b[0m hidden_state \u001b[38;5;241m=\u001b[39m distilbert_output[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# (bs, seq_len, dim)\u001b[39;00m\n\u001b[1;32m    988\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m hidden_state[:, \u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# (bs, dim)\u001b[39;00m\n",
      "File \u001b[0;32m~/ai_ml/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ai_ml/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/ai_ml/lib/python3.12/site-packages/transformers/models/distilbert/modeling_distilbert.py:785\u001b[0m, in \u001b[0;36mDistilBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[1;32m    783\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m--> 785\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[1;32m    787\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_flash_attention_2:\n\u001b[1;32m    788\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m attention_mask \u001b[38;5;28;01mif\u001b[39;00m (attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01min\u001b[39;00m attention_mask) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/ai_ml/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ai_ml/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/ai_ml/lib/python3.12/site-packages/transformers/models/distilbert/modeling_distilbert.py:116\u001b[0m, in \u001b[0;36mEmbeddings.forward\u001b[0;34m(self, input_ids, input_embeds)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03mParameters:\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;124;03m    input_ids (torch.Tensor):\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;124;03membeddings)\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 116\u001b[0m     input_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (bs, max_seq_length, dim)\u001b[39;00m\n\u001b[1;32m    118\u001b[0m seq_length \u001b[38;5;241m=\u001b[39m input_embeds\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    120\u001b[0m \u001b[38;5;66;03m# Setting the position-ids to the registered buffer in constructor, it helps\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m# when tracing the model without passing position-ids, solves\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# isues similar to issue #5664\u001b[39;00m\n",
      "File \u001b[0;32m~/ai_ml/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ai_ml/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/ai_ml/lib/python3.12/site-packages/torch/nn/modules/sparse.py:164\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ai_ml/lib/python3.12/site-packages/torch/nn/functional.py:2267\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2261\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2262\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2263\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2264\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2265\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2266\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2267\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at alloc_cpu.cpp:117] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 19660800000 bytes. Error code 12 (Cannot allocate memory)"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "sentences = []\n",
    "with open(\"5G_logs_v1.txt\", \"r\") as file:\n",
    "    for line in file:\n",
    "        sentences.append(line)\n",
    "        \n",
    "result = predict_sentiment(sentences)\n",
    "for sentence, verdict in zip(sentences, result):\n",
    "    logger.info(f\"{Style.BRIGHT}Verdict: {verdict}{Style.RESET_ALL}\")\n",
    "    logger.info(f\"{Style.BRIGHT}{Fore.BLUE}{sentence}{Style.RESET_ALL}\")\n",
    "    logger.info(f'\\n')\n",
    "end_time = time.time()\n",
    "logger.info(f'{Style.BRIGHT}{Fore.GREEN}Avg Time: {(end_time - start_time)/len(sentence)}{Style.RESET_ALL}')\n",
    "logger.info(f'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fdeaf1-632b-411f-833a-116ae45a7a20",
   "metadata": {},
   "source": [
    "### RUN 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7c8265-af04-46dc-8015-8718c5bd59fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "sentences = []\n",
    "with open(\"5G_logs_v2.txt\", \"r\") as file:\n",
    "    for line in file:\n",
    "        sentences.append(line)\n",
    "        \n",
    "result = predict_sentiment(sentences)\n",
    "for sentence, verdict in zip(sentences, result):\n",
    "    logger.info(f\"{Style.BRIGHT}Verdict: {verdict}{Style.RESET_ALL}\")\n",
    "    logger.info(f\"{Style.BRIGHT}{Fore.BLUE}{sentence}{Style.RESET_ALL}\")\n",
    "    logger.info(f'\\n')\n",
    "end_time = time.time()\n",
    "logger.info(f'{Style.BRIGHT}{Fore.GREEN}Avg Time: {(end_time - start_time)/len(sentence)}{Style.RESET_ALL}')\n",
    "logger.info(f'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0338614-6ee2-4f71-957f-60330c848eca",
   "metadata": {},
   "source": [
    "## Method 2: caching results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a355a74b-0a28-40a1-9708-9b0f4ba14c88",
   "metadata": {},
   "source": [
    "### **`Building cache`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "b5731a95-3c6b-4925-af3d-149e2802043a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "filenames = glob.glob(\"*.txt\")\n",
    "versions = []\n",
    "prefix = \"5G_logs_v\"\n",
    "for filename in filenames:\n",
    "    versions.append(int(filename.split(\".\")[0][len(prefix):]))\n",
    "versions.sort()\n",
    "latest = versions[-1]\n",
    "versions.pop(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "16326cbd-884d-405e-afa6-5d7932ba79dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache, sentence_list = defaultdict(list), []\n",
    "for filename in filenames:\n",
    "    fv = int(filename.split(\".\")[0][len(prefix):])\n",
    "    if fv != latest:\n",
    "        with open(f\"{filename}\", \"r\") as file:\n",
    "            for sentence in file:\n",
    "                sentence_list.append(sentence)\n",
    "results = predict_sentiment(sentence_list)\n",
    "for sentence, sentiment in zip(sentence_list, results):\n",
    "    insert_ = True\n",
    "    if len(cache[hash(sentence)]) != 0:\n",
    "        for sentenceX, sentiment in cache[hash(sentence)]:\n",
    "            if sentence == sentenceX:\n",
    "                insert_ = False\n",
    "                logging.debug(f\"{Style.BRIGHT}{Fore.RED}Duplicate{Style.RESET_ALL}\")\n",
    "                break\n",
    "    if insert_: cache[hash(sentence)].append((sentence, sentiment))\n",
    "\n",
    "with open(\"cache\", \"w\") as cfile:\n",
    "    json.dump(cache, cfile, indent = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56de82a-5d8f-45ae-aa9a-19019e2624f5",
   "metadata": {},
   "source": [
    "### RUN 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "0c36c8e6-2adb-478a-9475-dc6fe2e19d16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:\u001b[1mVerdict: NEUTRAL\u001b[0m\n",
      "INFO:root:\u001b[1m\u001b[34m[DEBUG] Everything is working properly.\n",
      "\u001b[0m\n",
      "INFO:root:\n",
      "\n",
      "INFO:root:\u001b[1mVerdict: NEUTRAL\u001b[0m\n",
      "INFO:root:\u001b[1m\u001b[34m[INFO] Is there a reason for this happening.\n",
      "\u001b[0m\n",
      "INFO:root:\n",
      "\n",
      "INFO:root:\u001b[1mVerdict: VERY BAD\u001b[0m\n",
      "INFO:root:\u001b[1m\u001b[34m[WARNING] This is really bad!!\n",
      "\u001b[0m\n",
      "INFO:root:\n",
      "\n",
      "INFO:root:\u001b[1mVerdict: VERY BAD\u001b[0m\n",
      "INFO:root:\u001b[1m\u001b[34m[ERROR] Very dangerous.\u001b[0m\n",
      "INFO:root:\n",
      "\n",
      "INFO:root:\u001b[1m\u001b[32mAvg Time: 0.012967161510301672\u001b[0m\n",
      "INFO:root:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "cache = {}\n",
    "with open(\"cache\", \"r\") as cfile:\n",
    "    cache = json.load(cfile)\n",
    "\n",
    "for filename in filenames:\n",
    "    fv = int(filename.split(\".\")[0][len(prefix):])\n",
    "    if fv == latest:\n",
    "        sentence_list, sentiment_list = [], []\n",
    "        with open(f\"{filename}\", \"r\") as file:\n",
    "            for sentence in file:\n",
    "                sentence_list.append(sentence)\n",
    "        sentiment_list = [None for _ in range(len(sentence))]\n",
    "        new_list, new_list_ids = [], []\n",
    "        for idx, sentence in enumerate(sentence_list):\n",
    "            hash_key, is_new = str(hash(sentence)), True\n",
    "            if hash_key in cache:\n",
    "                for sentenceX, sentiment in cache[hash_key]:\n",
    "                    if sentence == sentenceX: \n",
    "                        sentiment_list[idx] = sentiment \n",
    "                        is_new = False\n",
    "                        break\n",
    "            if is_new: \n",
    "                new_list.append(sentence)\n",
    "                new_list_ids.append(idx)\n",
    "        new_results = predict_sentiment(new_list)\n",
    "        for ptr, ids in enumerate(new_list_ids):\n",
    "            sentiment_list[ids] = new_results[ptr]\n",
    "        break \n",
    "        \n",
    "for sentence, verdict in zip(sentence_list, sentiment_list):\n",
    "    logger.info(f\"{Style.BRIGHT}Verdict: {verdict}{Style.RESET_ALL}\")\n",
    "    logger.info(f\"{Style.BRIGHT}{Fore.BLUE}{sentence}{Style.RESET_ALL}\")\n",
    "    logger.info(f'\\n')\n",
    "\n",
    "end_time = time.time()\n",
    "logger.info(f'{Style.BRIGHT}{Fore.GREEN}Avg Time: {(end_time - start_time)/len(sentence)}{Style.RESET_ALL}')\n",
    "logger.info(f'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ae93af-adbf-464b-a4da-1364a1729ea2",
   "metadata": {},
   "source": [
    "### RUN 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "76d6d464-bdb0-4b63-9259-b4b208168587",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:\u001b[1mVerdict: NEUTRAL\u001b[0m\n",
      "INFO:root:\u001b[1m\u001b[34m[DEBUG] Everything is working properly.\n",
      "\u001b[0m\n",
      "INFO:root:\n",
      "\n",
      "INFO:root:\u001b[1mVerdict: NEUTRAL\u001b[0m\n",
      "INFO:root:\u001b[1m\u001b[34m[INFO] Is there a reason for this happening.\n",
      "\u001b[0m\n",
      "INFO:root:\n",
      "\n",
      "INFO:root:\u001b[1mVerdict: VERY BAD\u001b[0m\n",
      "INFO:root:\u001b[1m\u001b[34m[WARNING] This is really bad!!\n",
      "\u001b[0m\n",
      "INFO:root:\n",
      "\n",
      "INFO:root:\u001b[1mVerdict: VERY BAD\u001b[0m\n",
      "INFO:root:\u001b[1m\u001b[34m[ERROR] Very dangerous.\u001b[0m\n",
      "INFO:root:\n",
      "\n",
      "INFO:root:\u001b[1m\u001b[32mAvg Time: 0.01152280102605405\u001b[0m\n",
      "INFO:root:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "cache = {}\n",
    "with open(\"cache\", \"r\") as cfile:\n",
    "    cache = json.load(cfile)\n",
    "\n",
    "for filename in filenames:\n",
    "    fv = int(filename.split(\".\")[0][len(prefix):])\n",
    "    if fv == latest:\n",
    "        sentence_list, sentiment_list = [], []\n",
    "        with open(f\"{filename}\", \"r\") as file:\n",
    "            for sentence in file:\n",
    "                sentence_list.append(sentence)\n",
    "        sentiment_list = [None for _ in range(len(sentence))]\n",
    "        new_list, new_list_ids = [], []\n",
    "        for idx, sentence in enumerate(sentence_list):\n",
    "            hash_key, is_new = str(hash(sentence)), True\n",
    "            if hash_key in cache:\n",
    "                for sentenceX, sentiment in cache[hash_key]:\n",
    "                    if sentence == sentenceX: \n",
    "                        sentiment_list[idx] = sentiment \n",
    "                        is_new = False\n",
    "                        break\n",
    "            if is_new: \n",
    "                new_list.append(sentence)\n",
    "                new_list_ids.append(idx)\n",
    "        new_results = predict_sentiment(new_list)\n",
    "        for ptr, ids in enumerate(new_list_ids):\n",
    "            sentiment_list[ids] = new_results[ptr]\n",
    "        break \n",
    "        \n",
    "for sentence, verdict in zip(sentence_list, sentiment_list):\n",
    "    logger.info(f\"{Style.BRIGHT}Verdict: {verdict}{Style.RESET_ALL}\")\n",
    "    logger.info(f\"{Style.BRIGHT}{Fore.BLUE}{sentence}{Style.RESET_ALL}\")\n",
    "    logger.info(f'\\n')\n",
    "\n",
    "end_time = time.time()\n",
    "logger.info(f'{Style.BRIGHT}{Fore.GREEN}Avg Time: {(end_time - start_time)/len(sentence)}{Style.RESET_ALL}')\n",
    "logger.info(f'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a081a384-fecf-4c14-a4a3-2a1036348b0f",
   "metadata": {},
   "source": [
    "### Expanding cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "1833c761-8ea9-4ac8-8085-13495fa9fc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence, verdict in zip(new_list, new_results):\n",
    "    hash_key = str(hash(sentence))\n",
    "    if hash_key not in cache:\n",
    "        cache[hash_key] = []\n",
    "    cache[hash_key].append((sentence, verdict))\n",
    "\n",
    "with open(\"cache\", \"w\") as cfile:\n",
    "    json.dump(cache, cfile, indent = 5)\n",
    "    new_list, new_results = [], []"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
