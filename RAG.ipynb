{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91615512-8d2d-4464-afca-c761d1c30305",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json \n",
    "import pickle\n",
    "import torch\n",
    "import os\n",
    "from colorama import Style, Fore, Back\n",
    "\n",
    "import warnings\n",
    "from sklearn.exceptions import InconsistentVersionWarning\n",
    "warnings.filterwarnings(\"ignore\", category=InconsistentVersionWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(\n",
    "    format=f'{Style.BRIGHT}{Fore.GREEN}%(levelname)s:%(asctime)s{Style.RESET_ALL} {Fore.BLUE}%(message)s{Style.RESET_ALL}', \n",
    "    level=logging.INFO\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4ded9e87-26e7-4aa9-80b5-5625cad20f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders  import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma \n",
    "from langchain_huggingface import HuggingFaceEmbeddings \n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from transformers import pipeline\n",
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "08c75c82-775e-46e9-8c59-416ac07316ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[32mINFO:2025-01-11 06:04:18,794\u001b[0m \u001b[34mSize of docx: 1\u001b[0m\n",
      "\u001b[1m\u001b[32mINFO:2025-01-11 06:04:18,800\u001b[0m \u001b[34m\n",
      "[DEBUG] Everything is working properly.\n",
      "[INFO] Is there a reason for this happening.\n",
      "[WARNING] This is really bad!!\n",
      "[ERROR] Very dangerous.\u001b[0m\n",
      "\u001b[1m\u001b[32mINFO:2025-01-11 06:04:18,803\u001b[0m \u001b[34mSize of splits: 1\u001b[0m\n",
      "\u001b[1m\u001b[32mINFO:2025-01-11 06:04:18,804\u001b[0m \u001b[34m\n",
      "[DEBUG] Everything is working properly.\n",
      "[INFO] Is there a reason for this happening.\n",
      "[WARNING] This is really bad!!\n",
      "[ERROR] Very dangerous.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# STEP 1 ~ Document Loader.\n",
    "loader = TextLoader(\"5G_logs_v2.txt\")\n",
    "docs = loader.load()\n",
    "logging.info(f'Size of docx: {len(docs)}')\n",
    "logging.info(f'\\n{docs[0].page_content}')\n",
    "\n",
    "# # STEP 2 ~ Split Documents.\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1000, # Each chunk will have 1000 characters\n",
    "    chunk_overlap = 200, # 200 characters will overlap from consecutive chunks\n",
    "    add_start_index = True # Output chunks will include a field that specifies the starting pos. in the orig docx.\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "logging.info(f'Size of splits: {len(all_splits)}')\n",
    "logging.info(f'\\n{all_splits[0].page_content}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "23997a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[32mINFO:2025-01-11 06:07:18,727\u001b[0m \u001b[34mLoad pretrained SentenceTransformer: sentence-transformers/all-mpnet-base-v2\u001b[0m\n",
      "\u001b[1m\u001b[32mWARNING:2025-01-11 06:08:49,501\u001b[0m \u001b[34mCreating vector-store from scratch.\u001b[0m\n",
      "\u001b[1m\u001b[32mINFO:2025-01-11 06:08:49,599\u001b[0m \u001b[34mAnonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\u001b[0m\n",
      "\u001b[1m\u001b[32mINFO:2025-01-11 06:08:50,930\u001b[0m \u001b[34m<langchain_chroma.vectorstores.Chroma object at 0x7f964a7d4470>\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model_embed_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': True}\n",
    "hf = HuggingFaceEmbeddings(model_name = model_embed_name, \\\n",
    "                           model_kwargs = model_kwargs, \\\n",
    "                           encode_kwargs = encode_kwargs)\n",
    "vectorstore, vector_db_dir = None, \"./5Gdb\"\n",
    "if os.path.exists(vector_db_dir):\n",
    "    vectorstore = Chroma(embedding_function = hf, \\\n",
    "                         persist_directory = vector_db_dir)\n",
    "else:\n",
    "    logging.warning(f'Creating vector-store from scratch.')\n",
    "    vectorstore = Chroma.from_documents(documents = all_splits, \\\n",
    "                                        embedding = hf, \\\n",
    "                                        persist_directory = vector_db_dir)\n",
    "logging.info(vectorstore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4c284636",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[32mINFO:2025-01-11 06:12:17,110\u001b[0m \u001b[34m\n",
      "[DEBUG] Everything is working properly.\n",
      "[INFO] Is there a reason for this happening.\n",
      "[WARNING] This is really bad!!\n",
      "[ERROR] Very dangerous.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 1})\n",
    "retrieved_docs = retriever.invoke(\"Display all [DEBUG] logs.\")\n",
    "logging.info(\"\\n\"+retrieved_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1843db1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "template0 = \"\"\"You are an AI assistant you job is to grade logs into three categories,\n",
    "BAD, NEUTRAL, GOOD.\n",
    "LOG --> {log}\n",
    "GRADE -->\"\"\"\n",
    "prompt = PromptTemplate(input = [\"log\"], template = template0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e1a79fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def QA(query):\n",
    "    # docs = retriever.invoke(query)\n",
    "    # context = \"\\n\".join(doc.page_content for doc in docs)\n",
    "    context = query\n",
    "    return {\"log\": context}\n",
    "\n",
    "# model_id = \"meta-llama/Llama-3.1-8B-Instruct\" -> 16GB model (slow as model cannot be fitted in RAM)\n",
    "# model_id = \"zackli4ai/llama-3.2-1b-instruct-qlora-int4-eo8\" -> (4GB but bad results)\n",
    "# model_id = \"meta-llama/Llama-3.2-1B-Instruct\" -> (2GB best one)\n",
    "def llm_resp(response, \\\n",
    "             model_name = \"Llama3.2-1B-Instruct/\", \\\n",
    "             additional_path = \"snapshots/9213176726f574b556790deb65791e0c5aa438b6/\"):\n",
    "    local_dir = \"/mnt/d/Desktop/HuggingFaceModels/meta-llama/\"\n",
    "    pipelineX = pipeline(\"text-generation\",\n",
    "                         model = local_dir + model_name + additional_path,\n",
    "                         model_kwargs = {\"torch_dtype\": torch.bfloat16},\n",
    "                         device_map = \"auto\",\n",
    "                         max_length = 128\n",
    "                        )\n",
    "    return pipelineX(response.text)[0]['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9fd9cc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_file(rag_chain, filename, cache_name):\n",
    "    cache = {}\n",
    "    try:\n",
    "      with open(f\"{cache_name}\", \"r\") as cfile:\n",
    "          cache = json.load(cfile)\n",
    "    except: pass\n",
    "    with open(f\"{filename}\", \"r\") as file:\n",
    "        for sentence in file:\n",
    "            hash_key = str(hash(sentence))\n",
    "            is_present = False\n",
    "            if hash_key in cache: \n",
    "              for sentenceX, verdict in cache.get(hash_key):\n",
    "                  if sentence == sentenceX:\n",
    "                      is_present = True \n",
    "                      break \n",
    "            if not is_present:\n",
    "                verdict = rag_chain.invoke(f\"{sentence}\")\n",
    "                if hash_key not in cache: cache[hash_key] = []\n",
    "                cache[hash_key].append([sentence, verdict])\n",
    "            logging.info(f\"\\n{sentence}\")\n",
    "            logging.info(f\"\\n{verdict}\")\n",
    "            logging.info(f'='*35)\n",
    "    with open(f\"{cache_name}\", \"w\") as cfile:\n",
    "        json.dump(cache, cfile, indent = 5)\n",
    "        cache = {} # RESET cache."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824798d8",
   "metadata": {},
   "source": [
    "## In-house Llama Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e0a1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    QA\n",
    "    | prompt\n",
    "    | llm_resp\n",
    ")\n",
    "parse_file(rag_chain, \"5G_logs_v1.txt\", \"cache_autoregressive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f3f9e6",
   "metadata": {},
   "source": [
    "## Different Models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dc5042dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[32mINFO:2025-01-12 07:21:17,569\u001b[0m \u001b[34m\n",
      "[DEBUG] Everything is working properly.\n",
      "\u001b[0m\n",
      "\u001b[1m\u001b[32mINFO:2025-01-12 07:21:17,573\u001b[0m \u001b[34m\n",
      " GOOD\u001b[0m\n",
      "\u001b[1m\u001b[32mINFO:2025-01-12 07:21:17,574\u001b[0m \u001b[34m===================================\u001b[0m\n",
      "\u001b[1m\u001b[32mINFO:2025-01-12 07:21:17,874\u001b[0m \u001b[34m\n",
      "[INFO] Is there a reason for this happening.\n",
      "\u001b[0m\n",
      "\u001b[1m\u001b[32mINFO:2025-01-12 07:21:17,875\u001b[0m \u001b[34m\n",
      " [NEUTRAL] No clear reason is apparent.\u001b[0m\n",
      "\u001b[1m\u001b[32mINFO:2025-01-12 07:21:17,876\u001b[0m \u001b[34m===================================\u001b[0m\n",
      "\u001b[1m\u001b[32mINFO:2025-01-12 07:21:18,155\u001b[0m \u001b[34m\n",
      "[WARNING] This is really bad!!\n",
      "\u001b[0m\n",
      "\u001b[1m\u001b[32mINFO:2025-01-12 07:21:18,157\u001b[0m \u001b[34m\n",
      " BAD\u001b[0m\n",
      "\u001b[1m\u001b[32mINFO:2025-01-12 07:21:18,158\u001b[0m \u001b[34m===================================\u001b[0m\n",
      "\u001b[1m\u001b[32mINFO:2025-01-12 07:21:18,160\u001b[0m \u001b[34m\n",
      "[DEBUG] Everything is working properly.\n",
      "\u001b[0m\n",
      "\u001b[1m\u001b[32mINFO:2025-01-12 07:21:18,161\u001b[0m \u001b[34m\n",
      " GOOD\u001b[0m\n",
      "\u001b[1m\u001b[32mINFO:2025-01-12 07:21:18,163\u001b[0m \u001b[34m===================================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def StopHallucinations(response):\n",
    "    return response.split(\"\\n\")[0]\n",
    "\n",
    "# model_id = \"openai-community/gpt2\"\n",
    "model_id = \"HuggingFaceH4/zephyr-7b-beta\"\n",
    "llm = HuggingFaceEndpoint(repo_id = model_id, temperature = 0.1)\n",
    "rag_chain = (\n",
    "    QA\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    "    | StopHallucinations\n",
    ")\n",
    "\n",
    "parse_file(rag_chain, \"5G_logs_v1.txt\", \"cache_autoreg_online\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5abb2963",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[32mINFO:2025-01-12 07:21:51,451\u001b[0m \u001b[34m\n",
      "[DEBUG] Everything is working properly.\n",
      "\u001b[0m\n",
      "\u001b[1m\u001b[32mINFO:2025-01-12 07:21:51,452\u001b[0m \u001b[34m\n",
      " GOOD\u001b[0m\n",
      "\u001b[1m\u001b[32mINFO:2025-01-12 07:21:51,453\u001b[0m \u001b[34m===================================\u001b[0m\n",
      "\u001b[1m\u001b[32mINFO:2025-01-12 07:21:52,243\u001b[0m \u001b[34m\n",
      "[INFO] Is there a reason for this happening.\n",
      "\u001b[0m\n",
      "\u001b[1m\u001b[32mINFO:2025-01-12 07:21:52,245\u001b[0m \u001b[34m\n",
      " NEUTRAL\u001b[0m\n",
      "\u001b[1m\u001b[32mINFO:2025-01-12 07:21:52,247\u001b[0m \u001b[34m===================================\u001b[0m\n",
      "\u001b[1m\u001b[32mINFO:2025-01-12 07:21:57,400\u001b[0m \u001b[34m\n",
      "[WARNING] This is really bad!!\n",
      "\u001b[0m\n",
      "\u001b[1m\u001b[32mINFO:2025-01-12 07:21:57,401\u001b[0m \u001b[34m\n",
      " BAD\u001b[0m\n",
      "\u001b[1m\u001b[32mINFO:2025-01-12 07:21:57,402\u001b[0m \u001b[34m===================================\u001b[0m\n",
      "\u001b[1m\u001b[32mINFO:2025-01-12 07:21:57,403\u001b[0m \u001b[34m\n",
      "[DEBUG] Everything is working properly.\n",
      "\u001b[0m\n",
      "\u001b[1m\u001b[32mINFO:2025-01-12 07:21:57,405\u001b[0m \u001b[34m\n",
      " GOOD\u001b[0m\n",
      "\u001b[1m\u001b[32mINFO:2025-01-12 07:21:57,406\u001b[0m \u001b[34m===================================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def StopHallucinations(response):\n",
    "     return response.split(\"\\n\")[0]\n",
    "\n",
    "model_id = \"microsoft/Phi-3.5-mini-instruct\"\n",
    "llm = HuggingFaceEndpoint(repo_id = model_id, temperature = 0.1)\n",
    "rag_chain = (\n",
    "    QA\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    "    | StopHallucinations\n",
    ")\n",
    "\n",
    "parse_file(rag_chain, \"5G_logs_v1.txt\", \"cache_autoreg_online\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
